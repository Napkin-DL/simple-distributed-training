{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Training using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Data](#Data)\n",
    "1. [Train](#Train)\n",
    "\n",
    "---\n",
    "\n",
    "## Background\n",
    "\n",
    "MNIST is a widely used dataset for handwritten digit classification. It consists of 70,000 labeled 28x28 pixel grayscale images of hand-written digits. The dataset is split into 60,000 training images and 10,000 test images. There are 10 classes (one for each of the 10 digits). This tutorial will show how to train and test an MNIST model on SageMaker using PyTorch.\n",
    "\n",
    "For more information about the PyTorch in SageMaker, please visit [sagemaker-pytorch-containers](https://github.com/aws/sagemaker-pytorch-containers) and [sagemaker-python-sdk](https://github.com/aws/sagemaker-python-sdk) github repositories.\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "_This notebook was created and tested on an ml.m4.xlarge notebook instance._\n",
    "\n",
    "Let's start by creating a SageMaker session and specifying:\n",
    "\n",
    "- The S3 bucket and prefix that you want to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting.\n",
    "- The IAM role arn used to give training and hosting access to your data. See the documentation for how to create these.  Note, if more than one role is required for notebook instances, training, and/or hosting, please replace the `sagemaker.get_execution_role()` with a the appropriate full IAM role arn string(s).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "\n",
    "from sagemaker.debugger import (\n",
    "    Rule, ProfilerRule, rule_configs, ProfilerConfig, \n",
    "    FrameworkProfile, DetailedProfilingConfig, \n",
    "    DataloaderProfilingConfig, PythonProfilingConfig)\n",
    "import time\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/DEMO-pytorch-mnist'\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.143.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input spec (in this case, just an S3 path): s3://sagemaker-us-west-2-322537213286/sagemaker/DEMO-pytorch-mnist\n"
     ]
    }
   ],
   "source": [
    "inputs = f\"s3://{bucket}/{prefix}\"\n",
    "print('input spec (in this case, just an S3 path): {}'.format(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "### Training script\n",
    "The script provides all the code we need for training and hosting a SageMaker model (`model_fn` function to load a model).\n",
    "The training script is very similar to a training script you might run outside of SageMaker, but you can access useful properties about the training environment through various environment variables, such as:\n",
    "\n",
    "* `SM_MODEL_DIR`: A string representing the path to the directory to write model artifacts to.\n",
    "  These artifacts are uploaded to S3 for model hosting.\n",
    "\n",
    "Supposing one input channel, 'training', was used in the call to the PyTorch estimator's `fit()` method, the following will be set, following the format `SM_CHANNEL_[channel_name]`:\n",
    "\n",
    "* `SM_CHANNEL_TRAINING`: A string representing the path to the directory containing data in the 'training' channel.\n",
    "\n",
    "For more information about training environment variables, please visit [SageMaker Containers](https://github.com/aws/sagemaker-containers).\n",
    "\n",
    "A typical training script loads data from the input channels, configures training with hyperparameters, trains a model, and saves a model to `model_dir` so that it can be hosted later. Hyperparameters are passed to your script as arguments and can be retrieved with an `argparse.ArgumentParser` instance.\n",
    "\n",
    "Because the SageMaker imports the training script, you should put your training code in a main guard (``if __name__=='__main__':``) if you are using the same script to host your model as we do in this example, so that SageMaker does not inadvertently run your training code at the wrong point in execution.\n",
    "\n",
    "For example, the script run by this notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Experiment\n",
    "Create or load [SageMaker Experiment](https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html) for the example training job. This will create an experiment trial object in SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_experiment(experiment_name):\n",
    "    try:\n",
    "        sm_experiment = Experiment.load(experiment_name)\n",
    "    except:\n",
    "        sm_experiment = Experiment.create(experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_trial(experiment_name):\n",
    "    from time import strftime\n",
    "    create_date = strftime(\"%m%d-%H%M%s\")\n",
    "    sm_trial = Trial.create(trial_name=f'{experiment_name}-{create_date}',\n",
    "                            experiment_name=experiment_name)\n",
    "\n",
    "    job_name = f'{sm_trial.trial_name}'\n",
    "    return job_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup fsx and use fsx for data channels and checkpoints\n",
    "\n",
    "While the above option is easier to setup, using an FSX can be beneficial for performance when dealing with large input sizes and large model sizes. \n",
    "\n",
    "Please see the instructions [here](https://github.com/aws/amazon-sagemaker-examples/blob/master/advanced_functionality/distributed_tensorflow_mask_rcnn/mask-rcnn-scriptmode-fsx.ipynb), to create the FSx lustre filesystem and import the dataset from the S3 bucket to your fsx filesystem. Note that the FSX must be created in a private subnet with internet gateway to ensure that training job has access to the internet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # # Instructions obtained from:\n",
    "\n",
    "# use_fsx = False\n",
    "\n",
    "# if use_fsx:\n",
    "#     from sagemaker.inputs import FileSystemInput\n",
    "\n",
    "#     # Specify FSx Lustre file system id.\n",
    "#     # file_system_id = \"<your-file-system-id>\"\n",
    "#     file_system_id = 'fs-099a02ed242945403'\n",
    "\n",
    "#     # Specify directory path for input data on the file system.\n",
    "#     # You need to provide normalized and absolute path below.\n",
    "#     # Your mount name can be provided by you when creating fsx, or generated automatically.\n",
    "#     # You can find this mount_name on the FSX page in console.\n",
    "#     # Example of fsx generated mount_name: \"3x5lhbmv\"\n",
    "#     # base_path = \"<your-mount-name>\"\n",
    "#     base_path = '/q7rzxbmv'\n",
    "\n",
    "#     # Specify your file system type.\n",
    "#     file_system_type = \"FSxLustre\"\n",
    "\n",
    "#     training = FileSystemInput(file_system_id=file_system_id,\n",
    "#                             file_system_type=file_system_type,\n",
    "#                             directory_path=base_path + \"/git-dataset\",\n",
    "#                             file_system_access_mode=\"rw\")\n",
    "#     model = FileSystemInput(file_system_id=file_system_id,\n",
    "#                             file_system_type=file_system_type,\n",
    "#                             directory_path=base_path + \"/bloom_model/bloom-560m\",\n",
    "#                             file_system_access_mode=\"rw\")\n",
    "\n",
    "#     data_channels = {\"training\": training, \"model\": model}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure rules\n",
    "We specify the following rules:\n",
    "- loss_not_decreasing: checks if loss is decreasing and triggers if the loss has not decreased by a certain persentage in the last few iterations\n",
    "- LowGPUUtilization: checks if GPU is under-utilizated \n",
    "- ProfilerReport: runs the entire set of performance rules and create a final output report with further insights and recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rules=[\n",
    "    ProfilerRule.sagemaker(rule_configs.ProfilerReport()),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Training Job with Profiling Enabled<a class=\"anchor\" id=\"option-1\"></a>\n",
    "\n",
    "You will use the standard [SageMaker Estimator API for PyTorch ](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/sagemaker.pytorch.html) to create training jobs. To enable profiling, create a `ProfilerConfig` object and pass it to the `profiler_config` parameter of the `PyTorch` estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "profiler_config=ProfilerConfig(\n",
    "    system_monitor_interval_millis=500,\n",
    "    framework_profile_params=FrameworkProfile(\n",
    "        start_step=5,num_steps=10,\n",
    "        detailed_profiling_config=DetailedProfilingConfig(start_step=5, num_steps=10),\n",
    "        dataloader_profiling_config=DataloaderProfilingConfig(start_step=5, num_steps=10),\n",
    "        python_profiling_config=PythonProfilingConfig(start_step=5, num_steps=10), # cprofile / Pyinstrument\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metric_definitions=[\n",
    "     {'Name': 'train:Loss', 'Regex': 'Train Loss: (.*?),'},\n",
    "     {'Name': 'test:Loss', 'Regex': 'Average loss: (.*?),'},\n",
    "     {'Name': 'test:Accuracy', 'Regex': 'Accuracy: (.*?)%'},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training with SMDataParallel\n",
    "\n",
    "\n",
    "The training script provides the code you need for distributed data parallel (DDP) training using SMDataParallel. The training script is very similar to a PyTorch training script you might run outside of SageMaker, but modified to run with SMDataParallel. SMDataParallel's PyTorch client provides an alternative to PyTorch's native DDP. For details about how to use SMDataParallel's DDP in your native PyTorch script, see the Getting Started with SMDataParallel tutorials.\n",
    "\n",
    "In the following code block, you can update the estimator function to use a different instance type, instance count, and distrubtion strategy. You're also passing in the training script you reviewed in the previous cell.\n",
    "\n",
    "**Instance types**\n",
    "\n",
    "SMDataParallel supports model training on SageMaker with the following instance types only:\n",
    "1. ml.p3.16xlarge\n",
    "1. ml.p3dn.24xlarge [Recommended]\n",
    "1. ml.p4d.24xlarge [Recommended]\n",
    "\n",
    "**Instance count**\n",
    "\n",
    "To get the best performance and the most out of SMDataParallel, you should use at least 2 instances, but you can also use 1 for testing this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'mnist-exp1'\n",
    "instance_type = 'ml.p3.16xlarge' # 'ml.p3.2xlarge'  # 'ml.p3.16xlarge', 'ml.p3dn.24xlarge', 'ml.p4d.24xlarge', 'local_gpu'\n",
    "instance_type = 'local_gpu'\n",
    "instance_count = 1\n",
    "use_spot_instances = False\n",
    "max_wait = None\n",
    "max_run = 1*60*60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if instance_type =='local_gpu':\n",
    "    from sagemaker.local import LocalSession\n",
    "    from pathlib import Path\n",
    "\n",
    "    sagemaker_session = LocalSession()\n",
    "    sagemaker_session.config = {'local': {'local_code': True}}\n",
    "    s3_data_path = f'file://{Path.cwd()}/data'\n",
    "    source_dir = f'{Path.cwd()}/train_code'\n",
    "else:\n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    s3_data_path = inputs\n",
    "    source_dir = 'train_code'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Distribution strategy**\n",
    "\n",
    "Note that to use DDP mode, you update the the `distribution` strategy, and set it to use `smdistributed dataparallel`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# distribution = {}\n",
    "\n",
    "# ### MPIRUN 수행\n",
    "# distribution[\"mpi\"]={\"enabled\": True}\n",
    "\n",
    "\n",
    "### SageMaker DDP\n",
    "# distribution[\"smdistributed\"] = {\n",
    "#     \"dataparallel\": {\"enabled\": True}\n",
    "# }\n",
    "\n",
    "# ### SageMaker MP\n",
    "# distribution['smdistributed'] = {\n",
    "#     \"modelparallel\": {\"enabled\":True,\n",
    "#                       \"parameters\": {\n",
    "#                           \"partitions\": mp_parameters['num_partitions'],\n",
    "#                           \"microbatches\": mp_parameters['num_microbatches'],\n",
    "#                           \"placement_strategy\": mp_parameters['placement_strategy'],\n",
    "#                           \"pipeline\": mp_parameters['pipeline'],\n",
    "#                           \"optimize\": mp_parameters['optimize'],\n",
    "#                           \"memory_weight\": mp_parameters['memory_weight'],\n",
    "#                           \"ddp\": mp_parameters['ddp']\n",
    "#                       }\n",
    "#                      }\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "distribution = {}\n",
    "\n",
    "flag = None\n",
    "# flag = 'ddp'\n",
    "# flag = 'smddp'\n",
    "\n",
    "if flag == 'ddp':\n",
    "    distribution[\"mpi\"]={\"enabled\": True}\n",
    "    # entry_point='pytorch_mnist_ddp.py'\n",
    "elif flag == 'smddp':\n",
    "    distribution[\"smdistributed\"] = {\"dataparallel\": {\"enabled\": True}}\n",
    "    # entry_point='pytorch_mnist_smddp.py'\n",
    "else:\n",
    "    distribution = None\n",
    "    # entry_point='pytorch_mnist_sm.py'\n",
    "distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point='pytorch_mnist_ddp.py',\n",
    "                    source_dir=source_dir,\n",
    "                    role=role,\n",
    "                    framework_version='1.13.1',\n",
    "                    py_version='py39',\n",
    "                    instance_count=instance_count,\n",
    "                    instance_type=instance_type,\n",
    "                    distribution=distribution,\n",
    "                    metric_definitions=metric_definitions,\n",
    "                    profiler_config=profiler_config,\n",
    "                    rules=rules,\n",
    "                    # disable_profiler=True,\n",
    "                    use_spot_instances=use_spot_instances,\n",
    "                    max_wait=max_wait,\n",
    "                    max_run=max_run,\n",
    "                    hyperparameters={\n",
    "                        'epochs': 5,\n",
    "                                    },\n",
    "                    sagemaker_session=sagemaker_session,\n",
    "                    tags=[{'Key':'Owner','Value':'username'}]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we've constructed our `PyTorch` object, we can fit it using the data we uploaded to S3. SageMaker makes sure our data is available in the local filesystem, so our training script can simply read the data from disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "INFO:sagemaker.image_uris:Defaulting to the only supported framework/algorithm version: latest.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: mnist-exp1-0403-06461680504401\n",
      "INFO:sagemaker.local.local_session:Starting training job\n",
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "INFO:botocore.credentials:Found credentials from IAM Role: BaseNotebookInstanceEc2InstanceRole\n",
      "INFO:sagemaker.local.image:No AWS credentials found in session but credentials from EC2 Metadata Service are available.\n",
      "INFO:sagemaker.local.image:docker compose file: \n",
      "networks:\n",
      "  sagemaker-local:\n",
      "    name: sagemaker-local\n",
      "services:\n",
      "  algo-1-75lkf:\n",
      "    command: train\n",
      "    container_name: hj714uvbww-algo-1-75lkf\n",
      "    deploy:\n",
      "      resources:\n",
      "        reservations:\n",
      "          devices:\n",
      "          - capabilities:\n",
      "            - gpu\n",
      "    environment:\n",
      "    - '[Masked]'\n",
      "    - '[Masked]'\n",
      "    image: 763104351884.dkr.ecr.us-west-2.amazonaws.com/pytorch-training:1.13.1-gpu-py39\n",
      "    networks:\n",
      "      sagemaker-local:\n",
      "        aliases:\n",
      "        - algo-1-75lkf\n",
      "    stdin_open: true\n",
      "    tty: true\n",
      "    volumes:\n",
      "    - /tmp/tmpfhjo4u3x/algo-1-75lkf/output:/opt/ml/output\n",
      "    - /tmp/tmpfhjo4u3x/algo-1-75lkf/input:/opt/ml/input\n",
      "    - /tmp/tmpfhjo4u3x/algo-1-75lkf/output/data:/opt/ml/output/data\n",
      "    - /tmp/tmpfhjo4u3x/model:/opt/ml/model\n",
      "    - /opt/ml/metadata:/opt/ml/metadata\n",
      "    - /home/ec2-user/SageMaker/2023/training-code/simple-distributed-training/data:/opt/ml/input/data/training\n",
      "    - /home/ec2-user/SageMaker/2023/training-code/simple-distributed-training/train_code:/opt/ml/code\n",
      "    - /tmp/tmpfhjo4u3x/shared:/opt/ml/shared\n",
      "version: '2.3'\n",
      "\n",
      "INFO:sagemaker.local.image:docker command: docker-compose -f /tmp/tmpfhjo4u3x/docker-compose.yaml up --build --abort-on-container-exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating hj714uvbww-algo-1-75lkf ... \n",
      "Creating hj714uvbww-algo-1-75lkf ... done\n",
      "Attaching to hj714uvbww-algo-1-75lkf\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 2023-04-03 06:46:44,029 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 2023-04-03 06:46:44,097 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 2023-04-03 06:46:44,107 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 2023-04-03 06:46:44,110 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 2023-04-03 06:46:44,113 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 2023-04-03 06:46:44,114 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m /opt/conda/bin/python3.9 -m pip install -r requirements.txt\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (1.5.3)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting lightning\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading lightning-2.0.1-py3-none-any.whl (1.8 MB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/1.8 MB 16.0 MB/s eta 0:00:00\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 1)) (2.8.2)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 1)) (2022.7.1)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Requirement already satisfied: numpy>=1.20.3 in /opt/conda/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 1)) (1.23.5)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting lightning-utilities<2.0,>=0.7.0\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading lightning_utilities-0.8.0-py3-none-any.whl (20 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Requirement already satisfied: PyYAML<8.0 in /opt/conda/lib/python3.9/site-packages (from lightning->-r requirements.txt (line 2)) (5.4.1)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting lightning-cloud>=0.5.31\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading lightning_cloud-0.5.32-py3-none-any.whl (545 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 546.0/546.0 kB 22.0 MB/s eta 0:00:00\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Requirement already satisfied: websocket-client<3.0 in /opt/conda/lib/python3.9/site-packages (from lightning->-r requirements.txt (line 2)) (1.5.1)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Requirement already satisfied: fsspec[http]<2025.0,>2021.06.0 in /opt/conda/lib/python3.9/site-packages (from lightning->-r requirements.txt (line 2)) (2023.1.0)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Requirement already satisfied: urllib3<3.0 in /opt/conda/lib/python3.9/site-packages (from lightning->-r requirements.txt (line 2)) (1.26.14)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Requirement already satisfied: traitlets<7.0,>=5.3.0 in /opt/conda/lib/python3.9/site-packages (from lightning->-r requirements.txt (line 2)) (5.9.0)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Requirement already satisfied: pydantic<3.0 in /opt/conda/lib/python3.9/site-packages (from lightning->-r requirements.txt (line 2)) (1.10.4)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting websockets<12.0\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading websockets-11.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129.5/129.5 kB 18.5 MB/s eta 0:00:00\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting dateutils<2.0\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading dateutils-0.6.12-py2.py3-none-any.whl (5.7 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting deepdiff<8.0,>=5.7.0\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading deepdiff-6.3.0-py3-none-any.whl (69 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 69.7/69.7 kB 10.7 MB/s eta 0:00:00\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting torchmetrics<2.0,>=0.7.0\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 519.2/519.2 kB 21.5 MB/s eta 0:00:00\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Requirement already satisfied: torch<4.0,>=1.11.0 in /opt/conda/lib/python3.9/site-packages (from lightning->-r requirements.txt (line 2)) (1.13.1+cu117)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting starsessions<2.0,>=1.2.1\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading starsessions-1.3.0-py3-none-any.whl (10 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Requirement already satisfied: packaging<25.0,>=17.1 in /opt/conda/lib/python3.9/site-packages (from lightning->-r requirements.txt (line 2)) (23.0)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting pytorch-lightning\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading pytorch_lightning-2.0.1-py3-none-any.whl (716 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 716.4/716.4 kB 26.0 MB/s eta 0:00:00\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Requirement already satisfied: rich<15.0,>=12.3.0 in /opt/conda/lib/python3.9/site-packages (from lightning->-r requirements.txt (line 2)) (12.6.0)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Requirement already satisfied: tqdm<6.0,>=4.57.0 in /opt/conda/lib/python3.9/site-packages (from lightning->-r requirements.txt (line 2)) (4.64.1)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Requirement already satisfied: click<10.0 in /opt/conda/lib/python3.9/site-packages (from lightning->-r requirements.txt (line 2)) (8.1.2)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Requirement already satisfied: requests<4.0 in /opt/conda/lib/python3.9/site-packages (from lightning->-r requirements.txt (line 2)) (2.28.2)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Requirement already satisfied: typing-extensions<6.0,>=4.0.0 in /opt/conda/lib/python3.9/site-packages (from lightning->-r requirements.txt (line 2)) (4.4.0)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting beautifulsoup4<6.0,>=4.8.0\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading beautifulsoup4-4.12.0-py3-none-any.whl (132 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 132.2/132.2 kB 21.2 MB/s eta 0:00:00\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting fastapi<0.89.0\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading fastapi-0.88.0-py3-none-any.whl (55 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 55.5/55.5 kB 9.6 MB/s eta 0:00:00\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting croniter<1.4.0,>=1.3.0\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading croniter-1.3.8-py2.py3-none-any.whl (18 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting arrow<3.0,>=1.2.0\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading arrow-1.2.3-py3-none-any.whl (66 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.4/66.4 kB 10.4 MB/s eta 0:00:00\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting starlette<2.0\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading starlette-0.26.1-py3-none-any.whl (66 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.9/66.9 kB 11.3 MB/s eta 0:00:00\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Requirement already satisfied: psutil<7.0 in /opt/conda/lib/python3.9/site-packages (from lightning->-r requirements.txt (line 2)) (5.9.4)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting uvicorn<2.0\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading uvicorn-0.21.1-py3-none-any.whl (57 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.8/57.8 kB 9.2 MB/s eta 0:00:00\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting inquirer<5.0,>=2.10.0\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading inquirer-3.1.3-py3-none-any.whl (18 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Requirement already satisfied: Jinja2<5.0 in /opt/conda/lib/python3.9/site-packages (from lightning->-r requirements.txt (line 2)) (3.1.2)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting soupsieve>1.2\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading soupsieve-2.4-py3-none-any.whl (37 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting ordered-set<4.2.0,>=4.0.2\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting starlette<2.0\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading starlette-0.22.0-py3-none-any.whl (64 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.3/64.3 kB 11.6 MB/s eta 0:00:00\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting anyio<5,>=3.4.0\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading anyio-3.6.2-py3-none-any.whl (80 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.6/80.6 kB 12.5 MB/s eta 0:00:00\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 30.6 MB/s eta 0:00:00\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting readchar>=3.0.6\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading readchar-4.0.5-py3-none-any.whl (8.5 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting python-editor>=1.0.4\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting blessed>=1.19.0\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading blessed-1.20.0-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.4/58.4 kB 9.6 MB/s eta 0:00:00\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from Jinja2<5.0->lightning->-r requirements.txt (line 2)) (2.1.2)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from lightning-cloud>=0.5.31->lightning->-r requirements.txt (line 2)) (1.16.0)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting fastapi[all]\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading fastapi-0.95.0-py3-none-any.whl (57 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.1/57.1 kB 10.1 MB/s eta 0:00:00\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting pyjwt\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading PyJWT-2.6.0-py3-none-any.whl (20 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<4.0->lightning->-r requirements.txt (line 2)) (2022.12.7)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<4.0->lightning->-r requirements.txt (line 2)) (3.4)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests<4.0->lightning->-r requirements.txt (line 2)) (2.1.1)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /opt/conda/lib/python3.9/site-packages (from rich<15.0,>=12.3.0->lightning->-r requirements.txt (line 2)) (2.14.0)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /opt/conda/lib/python3.9/site-packages (from rich<15.0,>=12.3.0->lightning->-r requirements.txt (line 2)) (0.9.1)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting itsdangerous<3.0.0,>=2.0.1\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting h11>=0.8\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 10.3 MB/s eta 0:00:00\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting aiosignal>=1.1.2\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting async-timeout<5.0,>=4.0.0a3\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning->-r requirements.txt (line 2)) (22.2.0)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting frozenlist>=1.1.1\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 158.8/158.8 kB 21.9 MB/s eta 0:00:00\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting yarl<2.0,>=1.0\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 264.6/264.6 kB 28.7 MB/s eta 0:00:00\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting multidict<7.0,>=4.5\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.2/114.2 kB 13.3 MB/s eta 0:00:00\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting sniffio>=1.1\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Requirement already satisfied: wcwidth>=0.1.4 in /opt/conda/lib/python3.9/site-packages (from blessed>=1.19.0->inquirer<5.0,>=2.10.0->lightning->-r requirements.txt (line 2)) (0.2.6)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Requirement already satisfied: setuptools>=41.0 in /opt/conda/lib/python3.9/site-packages (from readchar>=3.0.6->inquirer<5.0,>=2.10.0->lightning->-r requirements.txt (line 2)) (65.6.3)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting fastapi[all]\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading fastapi-0.94.1-py3-none-any.whl (56 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.4/56.4 kB 9.6 MB/s eta 0:00:00\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading fastapi-0.94.0-py3-none-any.whl (56 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.3/56.3 kB 7.9 MB/s eta 0:00:00\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading fastapi-0.93.0-py3-none-any.whl (56 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.3/56.3 kB 9.7 MB/s eta 0:00:00\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading fastapi-0.92.0-py3-none-any.whl (56 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.2/56.2 kB 9.7 MB/s eta 0:00:00\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading fastapi-0.91.0-py3-none-any.whl (56 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.2/56.2 kB 9.8 MB/s eta 0:00:00\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading fastapi-0.90.1-py3-none-any.whl (56 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.2/56.2 kB 8.8 MB/s eta 0:00:00\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading fastapi-0.90.0-py3-none-any.whl (56 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.0/56.0 kB 9.7 MB/s eta 0:00:00\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading fastapi-0.89.1-py3-none-any.whl (55 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 55.8/55.8 kB 9.3 MB/s eta 0:00:00\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading fastapi-0.89.0-py3-none-any.whl (55 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 55.6/55.6 kB 9.9 MB/s eta 0:00:00\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting python-multipart>=0.0.5\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 45.7/45.7 kB 7.6 MB/s eta 0:00:00\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting email-validator>=1.1.1\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading email_validator-1.3.1-py2.py3-none-any.whl (22 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting orjson>=3.2.1\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading orjson-3.8.9-cp39-cp39-manylinux_2_28_x86_64.whl (144 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 144.1/144.1 kB 20.7 MB/s eta 0:00:00\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading ujson-5.7.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.8/52.8 kB 8.9 MB/s eta 0:00:00\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting httpx>=0.23.0\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading httpx-0.23.3-py3-none-any.whl (71 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71.5/71.5 kB 12.3 MB/s eta 0:00:00\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting dnspython>=1.15.0\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading dnspython-2.3.0-py3-none-any.whl (283 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 283.7/283.7 kB 27.2 MB/s eta 0:00:00\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting rfc3986[idna2008]<2,>=1.3\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting httpcore<0.17.0,>=0.15.0\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading httpcore-0.16.3-py3-none-any.whl (69 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 69.6/69.6 kB 12.2 MB/s eta 0:00:00\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading uvloop-0.17.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.2/4.2 MB 41.8 MB/s eta 0:00:00\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting python-dotenv>=0.13\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting httptools>=0.5.0\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading httptools-0.5.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (417 kB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 417.9/417.9 kB 42.9 MB/s eta 0:00:00\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Collecting watchfiles>=0.13\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading watchfiles-0.19.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 48.2 MB/s eta 0:00:00\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Installing collected packages: rfc3986, python-editor, websockets, uvloop, ujson, soupsieve, sniffio, readchar, python-multipart, python-dotenv, pyjwt, orjson, ordered-set, multidict, lightning-utilities, itsdangerous, httptools, h11, frozenlist, dnspython, blessed, async-timeout, yarl, uvicorn, torchmetrics, inquirer, email-validator, deepdiff, dateutils, croniter, beautifulsoup4, arrow, anyio, aiosignal, watchfiles, starlette, httpcore, aiohttp, starsessions, httpx, fastapi, pytorch-lightning, lightning-cloud, lightning\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 anyio-3.6.2 arrow-1.2.3 async-timeout-4.0.2 beautifulsoup4-4.12.0 blessed-1.20.0 croniter-1.3.8 dateutils-0.6.12 deepdiff-6.3.0 dnspython-2.3.0 email-validator-1.3.1 fastapi-0.88.0 frozenlist-1.3.3 h11-0.14.0 httpcore-0.16.3 httptools-0.5.0 httpx-0.23.3 inquirer-3.1.3 itsdangerous-2.1.2 lightning-2.0.1 lightning-cloud-0.5.32 lightning-utilities-0.8.0 multidict-6.0.4 ordered-set-4.1.0 orjson-3.8.9 pyjwt-2.6.0 python-dotenv-1.0.0 python-editor-1.0.4 python-multipart-0.0.6 pytorch-lightning-2.0.1 readchar-4.0.5 rfc3986-1.5.0 sniffio-1.3.0 soupsieve-2.4 starlette-0.22.0 starsessions-1.3.0 torchmetrics-0.11.4 ujson-5.7.0 uvicorn-0.21.1 uvloop-0.17.0 watchfiles-0.19.0 websockets-11.0 yarl-1.8.2\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m [notice] A new release of pip is available: 23.0 -> 23.0.1\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m [notice] To update, run: pip install --upgrade pip\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 2023-04-03 06:46:57,762 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 2023-04-03 06:46:57,762 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 2023-04-03 06:46:57,833 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 2023-04-03 06:46:57,843 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 2023-04-03 06:46:57,917 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 2023-04-03 06:46:57,928 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 2023-04-03 06:46:57,998 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 2023-04-03 06:46:58,022 sagemaker-training-toolkit INFO     instance_groups entry not present in resource_config\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 2023-04-03 06:46:58,026 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Training Env:\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m {\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m     \"additional_framework_parameters\": {},\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m     \"channel_input_dirs\": {\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m         \"training\": \"/opt/ml/input/data/training\"\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m     },\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m     \"current_host\": \"algo-1-75lkf\",\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m     \"current_instance_group\": \"homogeneousCluster\",\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m     \"current_instance_group_hosts\": [],\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m     \"current_instance_type\": \"local\",\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m     \"distribution_hosts\": [\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m         \"algo-1-75lkf\"\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m     ],\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m     \"distribution_instance_groups\": [],\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m     \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m     \"hosts\": [\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m         \"algo-1-75lkf\"\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m     ],\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m     \"hyperparameters\": {\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m         \"epochs\": 5\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m     },\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m     \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m     \"input_data_config\": {\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m         \"training\": {\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m             \"TrainingInputMode\": \"File\"\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m         }\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m     },\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m     \"input_dir\": \"/opt/ml/input\",\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m     \"instance_groups\": [],\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m     \"instance_groups_dict\": {},\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m     \"is_hetero\": false,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m     \"is_master\": true,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m     \"is_modelparallel_enabled\": null,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m     \"is_smddpmprun_installed\": true,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m     \"job_name\": \"mnist-exp1-0403-06461680504401\",\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m     \"log_level\": 20,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m     \"master_hostname\": \"algo-1-75lkf\",\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m     \"model_dir\": \"/opt/ml/model\",\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m     \"module_dir\": \"/opt/ml/code\",\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m     \"module_name\": \"pytorch_mnist\",\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m     \"network_interface_name\": \"eth0\",\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m     \"num_cpus\": 64,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m     \"num_gpus\": 8,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m     \"num_neurons\": 0,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m     \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m     \"output_dir\": \"/opt/ml/output\",\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m     \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m     \"resource_config\": {\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m         \"current_host\": \"algo-1-75lkf\",\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m         \"hosts\": [\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m             \"algo-1-75lkf\"\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m         ]\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m     },\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m     \"user_entry_point\": \"pytorch_mnist.py\"\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m }\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Environment variables:\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m SM_HOSTS=[\"algo-1-75lkf\"]\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m SM_NETWORK_INTERFACE_NAME=eth0\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m SM_HPS={\"epochs\":5}\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m SM_USER_ENTRY_POINT=pytorch_mnist.py\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m SM_FRAMEWORK_PARAMS={}\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m SM_RESOURCE_CONFIG={\"current_host\":\"algo-1-75lkf\",\"hosts\":[\"algo-1-75lkf\"]}\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m SM_INPUT_DATA_CONFIG={\"training\":{\"TrainingInputMode\":\"File\"}}\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m SM_CHANNELS=[\"training\"]\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m SM_CURRENT_HOST=algo-1-75lkf\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m SM_CURRENT_INSTANCE_TYPE=local\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m SM_CURRENT_INSTANCE_GROUP_HOSTS=[]\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m SM_INSTANCE_GROUPS=[]\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m SM_INSTANCE_GROUPS_DICT={}\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m SM_DISTRIBUTION_INSTANCE_GROUPS=[]\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m SM_IS_HETERO=false\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m SM_MODULE_NAME=pytorch_mnist\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m SM_LOG_LEVEL=20\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m SM_INPUT_DIR=/opt/ml/input\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m SM_OUTPUT_DIR=/opt/ml/output\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m SM_NUM_CPUS=64\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m SM_NUM_GPUS=8\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m SM_NUM_NEURONS=0\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m SM_MODEL_DIR=/opt/ml/model\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m SM_MODULE_DIR=/opt/ml/code\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1-75lkf\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[],\"current_instance_type\":\"local\",\"distribution_hosts\":[\"algo-1-75lkf\"],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1-75lkf\"],\"hyperparameters\":{\"epochs\":5},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[],\"instance_groups_dict\":{},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"mnist-exp1-0403-06461680504401\",\"log_level\":20,\"master_hostname\":\"algo-1-75lkf\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/code\",\"module_name\":\"pytorch_mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1-75lkf\",\"hosts\":[\"algo-1-75lkf\"]},\"user_entry_point\":\"pytorch_mnist.py\"}\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m SM_USER_ARGS=[\"--epochs\",\"5\"]\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m SM_CHANNEL_TRAINING=/opt/ml/input/data/training\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m SM_HP_EPOCHS=5\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Invoking script with the following command:\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m /opt/conda/bin/python3.9 pytorch_mnist.py --epochs 5\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 2023-04-03 06:46:59,692 root         INFO     Using NamedTuple = typing._NamedTuple instead.\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 2023-04-03 06:46:59,886 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/train-images-idx3-ubyte.gz\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 0%|          | 0/9912422 [00:00<?, ?it/s]\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 1%|          | 65536/9912422 [00:00<00:18, 529852.44it/s]\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 2%|▏         | 229376/9912422 [00:00<00:09, 1010294.54it/s]\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 4%|▍         | 425984/9912422 [00:00<00:07, 1268490.57it/s]\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 6%|▋         | 622592/9912422 [00:00<00:06, 1397931.90it/s]\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 8%|▊         | 819200/9912422 [00:00<00:05, 1551572.30it/s]\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 11%|█         | 1081344/9912422 [00:00<00:05, 1667064.28it/s]\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 14%|█▎        | 1343488/9912422 [00:00<00:04, 1823894.32it/s]\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 17%|█▋        | 1638400/9912422 [00:00<00:04, 2003323.84it/s]\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 20%|█▉        | 1966080/9912422 [00:01<00:03, 2206355.76it/s]\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 23%|██▎       | 2326528/9912422 [00:01<00:03, 2426544.48it/s]\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 27%|██▋       | 2719744/9912422 [00:01<00:02, 2658645.24it/s]\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 32%|███▏      | 3178496/9912422 [00:01<00:02, 2972685.14it/s]\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 37%|███▋      | 3637248/9912422 [00:01<00:01, 3208679.38it/s]\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 42%|████▏     | 4161536/9912422 [00:01<00:01, 3523898.22it/s]\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 48%|████▊     | 4751360/9912422 [00:01<00:01, 3901500.93it/s]\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 55%|█████▍    | 5406720/9912422 [00:01<00:01, 4329123.40it/s]\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 61%|██████▏   | 6094848/9912422 [00:02<00:00, 4914529.65it/s]\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 69%|██████▉   | 6881280/9912422 [00:02<00:00, 5365446.65it/s]\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 78%|███████▊  | 7766016/9912422 [00:02<00:00, 5657411.95it/s]\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 88%|████████▊ | 8716288/9912422 [00:02<00:00, 6270663.83it/s]\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 99%|█████████▊| 9764864/9912422 [00:02<00:00, 6926844.67it/s]\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 100%|██████████| 9912422/9912422 [00:02<00:00, 3824030.40it/s]\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/train-labels-idx1-ubyte.gz\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 0%|          | 0/28881 [00:00<?, ?it/s]\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 100%|██████████| 28881/28881 [00:00<00:00, 460009.55it/s]\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/t10k-images-idx3-ubyte.gz\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 0%|          | 0/1648877 [00:00<?, ?it/s]\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 4%|▍         | 65536/1648877 [00:00<00:03, 522474.30it/s]\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 16%|█▌        | 262144/1648877 [00:00<00:01, 1134317.90it/s]\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 64%|██████▎   | 1048576/1648877 [00:00<00:00, 3469459.77it/s]\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 100%|██████████| 1648877/1648877 [00:00<00:00, 4332152.39it/s]\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/t10k-labels-idx1-ubyte.gz\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Downloading https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 0%|          | 0/4542 [00:00<?, ?it/s]\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 100%|██████████| 4542/4542 [00:00<00:00, 4248556.82it/s]\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m \n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m INFO:root:Using NamedTuple = typing._NamedTuple instead.\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m [2023-04-03 06:47:13.047 algo-1-75lkf:145 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m [2023-04-03 06:47:13.460 algo-1-75lkf:145 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m /opt/conda/lib/python3.9/site-packages/torch/nn/functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m   warnings.warn(warn_msg)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [0/60000 (0%)]\t - Train Loss: 2.306931,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [640/60000 (1%)]\t - Train Loss: 1.490004,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [1280/60000 (2%)]\t - Train Loss: 0.770381,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [1920/60000 (3%)]\t - Train Loss: 0.445672,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [2560/60000 (4%)]\t - Train Loss: 0.447884,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [3200/60000 (5%)]\t - Train Loss: 0.332245,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [3840/60000 (6%)]\t - Train Loss: 0.282164,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [4480/60000 (7%)]\t - Train Loss: 0.292138,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [5120/60000 (9%)]\t - Train Loss: 0.466773,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [5760/60000 (10%)]\t - Train Loss: 0.236672,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [6400/60000 (11%)]\t - Train Loss: 0.270311,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [7040/60000 (12%)]\t - Train Loss: 0.189321,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [7680/60000 (13%)]\t - Train Loss: 0.214764,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [8320/60000 (14%)]\t - Train Loss: 0.146612,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [8960/60000 (15%)]\t - Train Loss: 0.269608,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [9600/60000 (16%)]\t - Train Loss: 0.156269,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [10240/60000 (17%)]\t - Train Loss: 0.306638,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [10880/60000 (18%)]\t - Train Loss: 0.205494,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [11520/60000 (19%)]\t - Train Loss: 0.478145,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [12160/60000 (20%)]\t - Train Loss: 0.298962,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [12800/60000 (21%)]\t - Train Loss: 0.103555,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [13440/60000 (22%)]\t - Train Loss: 0.120663,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [14080/60000 (23%)]\t - Train Loss: 0.153480,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [14720/60000 (25%)]\t - Train Loss: 0.738398,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [15360/60000 (26%)]\t - Train Loss: 0.181646,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [16000/60000 (27%)]\t - Train Loss: 0.233567,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [16640/60000 (28%)]\t - Train Loss: 0.278258,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [17280/60000 (29%)]\t - Train Loss: 0.043562,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [17920/60000 (30%)]\t - Train Loss: 0.157447,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [18560/60000 (31%)]\t - Train Loss: 0.209457,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [19200/60000 (32%)]\t - Train Loss: 0.312620,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [19840/60000 (33%)]\t - Train Loss: 0.210998,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [20480/60000 (34%)]\t - Train Loss: 0.019293,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [21120/60000 (35%)]\t - Train Loss: 0.241887,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [21760/60000 (36%)]\t - Train Loss: 0.026581,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [22400/60000 (37%)]\t - Train Loss: 0.122029,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [23040/60000 (38%)]\t - Train Loss: 0.206977,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [23680/60000 (39%)]\t - Train Loss: 0.200061,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [24320/60000 (41%)]\t - Train Loss: 0.014559,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [24960/60000 (42%)]\t - Train Loss: 0.162878,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [25600/60000 (43%)]\t - Train Loss: 0.307013,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [26240/60000 (44%)]\t - Train Loss: 0.073340,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [26880/60000 (45%)]\t - Train Loss: 0.359264,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [27520/60000 (46%)]\t - Train Loss: 0.300282,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [28160/60000 (47%)]\t - Train Loss: 0.158003,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [28800/60000 (48%)]\t - Train Loss: 0.203581,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [29440/60000 (49%)]\t - Train Loss: 0.104874,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [30080/60000 (50%)]\t - Train Loss: 0.200684,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [30720/60000 (51%)]\t - Train Loss: 0.063204,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [31360/60000 (52%)]\t - Train Loss: 0.121354,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [32000/60000 (53%)]\t - Train Loss: 0.213264,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [32640/60000 (54%)]\t - Train Loss: 0.241141,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [33280/60000 (55%)]\t - Train Loss: 0.223515,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [33920/60000 (57%)]\t - Train Loss: 0.046004,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [34560/60000 (58%)]\t - Train Loss: 0.086221,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [35200/60000 (59%)]\t - Train Loss: 0.187357,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [35840/60000 (60%)]\t - Train Loss: 0.149465,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [36480/60000 (61%)]\t - Train Loss: 0.089485,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [37120/60000 (62%)]\t - Train Loss: 0.146330,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [37760/60000 (63%)]\t - Train Loss: 0.203554,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [38400/60000 (64%)]\t - Train Loss: 0.156198,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [39040/60000 (65%)]\t - Train Loss: 0.018822,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [39680/60000 (66%)]\t - Train Loss: 0.062101,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [40320/60000 (67%)]\t - Train Loss: 0.127695,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [40960/60000 (68%)]\t - Train Loss: 0.066802,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [41600/60000 (69%)]\t - Train Loss: 0.059383,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [42240/60000 (70%)]\t - Train Loss: 0.026597,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [42880/60000 (71%)]\t - Train Loss: 0.172450,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [43520/60000 (72%)]\t - Train Loss: 0.198327,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [44160/60000 (74%)]\t - Train Loss: 0.027026,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [44800/60000 (75%)]\t - Train Loss: 0.168582,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [45440/60000 (76%)]\t - Train Loss: 0.106804,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [46080/60000 (77%)]\t - Train Loss: 0.238595,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [46720/60000 (78%)]\t - Train Loss: 0.127883,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [47360/60000 (79%)]\t - Train Loss: 0.103168,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [48000/60000 (80%)]\t - Train Loss: 0.129606,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [48640/60000 (81%)]\t - Train Loss: 0.020504,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [49280/60000 (82%)]\t - Train Loss: 0.043886,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [49920/60000 (83%)]\t - Train Loss: 0.093003,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [50560/60000 (84%)]\t - Train Loss: 0.042874,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [51200/60000 (85%)]\t - Train Loss: 0.099580,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [51840/60000 (86%)]\t - Train Loss: 0.026544,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [52480/60000 (87%)]\t - Train Loss: 0.060383,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [53120/60000 (88%)]\t - Train Loss: 0.227783,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [53760/60000 (90%)]\t - Train Loss: 0.165488,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [54400/60000 (91%)]\t - Train Loss: 0.017989,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [55040/60000 (92%)]\t - Train Loss: 0.014816,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [55680/60000 (93%)]\t - Train Loss: 0.105643,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [56320/60000 (94%)]\t - Train Loss: 0.025218,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [56960/60000 (95%)]\t - Train Loss: 0.047037,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [57600/60000 (96%)]\t - Train Loss: 0.140906,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [58240/60000 (97%)]\t - Train Loss: 0.068971,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [58880/60000 (98%)]\t - Train Loss: 0.028009,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 1 [59520/60000 (99%)]\t - Train Loss: 0.018791,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Test set: Average loss: 0.0503, Accuracy: 98.37% (9837/10000)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [0/60000 (0%)]\t - Train Loss: 0.035056,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [640/60000 (1%)]\t - Train Loss: 0.048172,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [1280/60000 (2%)]\t - Train Loss: 0.119510,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [1920/60000 (3%)]\t - Train Loss: 0.134853,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [2560/60000 (4%)]\t - Train Loss: 0.029392,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [3200/60000 (5%)]\t - Train Loss: 0.103685,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [3840/60000 (6%)]\t - Train Loss: 0.028686,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [4480/60000 (7%)]\t - Train Loss: 0.050412,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [5120/60000 (9%)]\t - Train Loss: 0.218523,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [5760/60000 (10%)]\t - Train Loss: 0.131358,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [6400/60000 (11%)]\t - Train Loss: 0.143300,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [7040/60000 (12%)]\t - Train Loss: 0.193060,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [7680/60000 (13%)]\t - Train Loss: 0.020500,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [8320/60000 (14%)]\t - Train Loss: 0.029505,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [8960/60000 (15%)]\t - Train Loss: 0.136861,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [9600/60000 (16%)]\t - Train Loss: 0.085656,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [10240/60000 (17%)]\t - Train Loss: 0.063096,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [10880/60000 (18%)]\t - Train Loss: 0.020311,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [11520/60000 (19%)]\t - Train Loss: 0.092456,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [12160/60000 (20%)]\t - Train Loss: 0.086242,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [12800/60000 (21%)]\t - Train Loss: 0.083071,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [13440/60000 (22%)]\t - Train Loss: 0.027031,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [14080/60000 (23%)]\t - Train Loss: 0.016589,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [14720/60000 (25%)]\t - Train Loss: 0.129371,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [15360/60000 (26%)]\t - Train Loss: 0.045420,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [16000/60000 (27%)]\t - Train Loss: 0.104840,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [16640/60000 (28%)]\t - Train Loss: 0.134251,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [17280/60000 (29%)]\t - Train Loss: 0.019174,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [17920/60000 (30%)]\t - Train Loss: 0.041493,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [18560/60000 (31%)]\t - Train Loss: 0.079873,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [19200/60000 (32%)]\t - Train Loss: 0.063624,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [19840/60000 (33%)]\t - Train Loss: 0.122639,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [20480/60000 (34%)]\t - Train Loss: 0.086185,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [21120/60000 (35%)]\t - Train Loss: 0.138904,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [21760/60000 (36%)]\t - Train Loss: 0.011643,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [22400/60000 (37%)]\t - Train Loss: 0.012205,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [23040/60000 (38%)]\t - Train Loss: 0.080088,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [23680/60000 (39%)]\t - Train Loss: 0.189951,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [24320/60000 (41%)]\t - Train Loss: 0.000989,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [24960/60000 (42%)]\t - Train Loss: 0.034675,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [25600/60000 (43%)]\t - Train Loss: 0.096180,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [26240/60000 (44%)]\t - Train Loss: 0.050407,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [26880/60000 (45%)]\t - Train Loss: 0.194280,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [27520/60000 (46%)]\t - Train Loss: 0.114987,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [28160/60000 (47%)]\t - Train Loss: 0.024600,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [28800/60000 (48%)]\t - Train Loss: 0.036679,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [29440/60000 (49%)]\t - Train Loss: 0.044397,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [30080/60000 (50%)]\t - Train Loss: 0.047392,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [30720/60000 (51%)]\t - Train Loss: 0.043439,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [31360/60000 (52%)]\t - Train Loss: 0.050506,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [32000/60000 (53%)]\t - Train Loss: 0.284647,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [32640/60000 (54%)]\t - Train Loss: 0.129431,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [33280/60000 (55%)]\t - Train Loss: 0.149185,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [33920/60000 (57%)]\t - Train Loss: 0.012863,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [34560/60000 (58%)]\t - Train Loss: 0.022135,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [35200/60000 (59%)]\t - Train Loss: 0.083477,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [35840/60000 (60%)]\t - Train Loss: 0.113762,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [36480/60000 (61%)]\t - Train Loss: 0.045363,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [37120/60000 (62%)]\t - Train Loss: 0.043022,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [37760/60000 (63%)]\t - Train Loss: 0.168615,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [38400/60000 (64%)]\t - Train Loss: 0.118570,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [39040/60000 (65%)]\t - Train Loss: 0.010852,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [39680/60000 (66%)]\t - Train Loss: 0.069331,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [40320/60000 (67%)]\t - Train Loss: 0.033320,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [40960/60000 (68%)]\t - Train Loss: 0.084166,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [41600/60000 (69%)]\t - Train Loss: 0.034301,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [42240/60000 (70%)]\t - Train Loss: 0.035928,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [42880/60000 (71%)]\t - Train Loss: 0.027276,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [43520/60000 (72%)]\t - Train Loss: 0.050201,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [44160/60000 (74%)]\t - Train Loss: 0.001816,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [44800/60000 (75%)]\t - Train Loss: 0.054816,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [45440/60000 (76%)]\t - Train Loss: 0.135835,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [46080/60000 (77%)]\t - Train Loss: 0.140216,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [46720/60000 (78%)]\t - Train Loss: 0.130585,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [47360/60000 (79%)]\t - Train Loss: 0.077243,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [48000/60000 (80%)]\t - Train Loss: 0.145941,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [48640/60000 (81%)]\t - Train Loss: 0.072428,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [49280/60000 (82%)]\t - Train Loss: 0.011577,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [49920/60000 (83%)]\t - Train Loss: 0.057909,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [50560/60000 (84%)]\t - Train Loss: 0.016450,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [51200/60000 (85%)]\t - Train Loss: 0.185110,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [51840/60000 (86%)]\t - Train Loss: 0.005723,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [52480/60000 (87%)]\t - Train Loss: 0.007982,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [53120/60000 (88%)]\t - Train Loss: 0.125215,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [53760/60000 (90%)]\t - Train Loss: 0.089692,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [54400/60000 (91%)]\t - Train Loss: 0.065687,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [55040/60000 (92%)]\t - Train Loss: 0.005501,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [55680/60000 (93%)]\t - Train Loss: 0.129212,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [56320/60000 (94%)]\t - Train Loss: 0.025442,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [56960/60000 (95%)]\t - Train Loss: 0.027320,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [57600/60000 (96%)]\t - Train Loss: 0.131173,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [58240/60000 (97%)]\t - Train Loss: 0.004434,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [58880/60000 (98%)]\t - Train Loss: 0.002071,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 2 [59520/60000 (99%)]\t - Train Loss: 0.002435,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Test set: Average loss: 0.0390, Accuracy: 98.67% (9867/10000)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [0/60000 (0%)]\t - Train Loss: 0.053645,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [640/60000 (1%)]\t - Train Loss: 0.011757,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [1280/60000 (2%)]\t - Train Loss: 0.067646,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [1920/60000 (3%)]\t - Train Loss: 0.075158,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [2560/60000 (4%)]\t - Train Loss: 0.018741,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [3200/60000 (5%)]\t - Train Loss: 0.038905,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [3840/60000 (6%)]\t - Train Loss: 0.021100,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [4480/60000 (7%)]\t - Train Loss: 0.070193,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [5120/60000 (9%)]\t - Train Loss: 0.104921,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [5760/60000 (10%)]\t - Train Loss: 0.100771,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [6400/60000 (11%)]\t - Train Loss: 0.137405,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [7040/60000 (12%)]\t - Train Loss: 0.209402,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [7680/60000 (13%)]\t - Train Loss: 0.079083,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [8320/60000 (14%)]\t - Train Loss: 0.029974,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [8960/60000 (15%)]\t - Train Loss: 0.095878,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [9600/60000 (16%)]\t - Train Loss: 0.086567,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [10240/60000 (17%)]\t - Train Loss: 0.129119,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [10880/60000 (18%)]\t - Train Loss: 0.015688,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [11520/60000 (19%)]\t - Train Loss: 0.040152,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [12160/60000 (20%)]\t - Train Loss: 0.042258,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [12800/60000 (21%)]\t - Train Loss: 0.083614,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [13440/60000 (22%)]\t - Train Loss: 0.074465,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [14080/60000 (23%)]\t - Train Loss: 0.062734,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [14720/60000 (25%)]\t - Train Loss: 0.064982,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [15360/60000 (26%)]\t - Train Loss: 0.019931,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [16000/60000 (27%)]\t - Train Loss: 0.144660,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [16640/60000 (28%)]\t - Train Loss: 0.156174,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [17280/60000 (29%)]\t - Train Loss: 0.026502,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [17920/60000 (30%)]\t - Train Loss: 0.012077,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [18560/60000 (31%)]\t - Train Loss: 0.020572,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [19200/60000 (32%)]\t - Train Loss: 0.057321,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [19840/60000 (33%)]\t - Train Loss: 0.068355,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [20480/60000 (34%)]\t - Train Loss: 0.016284,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [21120/60000 (35%)]\t - Train Loss: 0.056243,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [21760/60000 (36%)]\t - Train Loss: 0.045590,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [22400/60000 (37%)]\t - Train Loss: 0.012530,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [23040/60000 (38%)]\t - Train Loss: 0.083944,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [23680/60000 (39%)]\t - Train Loss: 0.109863,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [24320/60000 (41%)]\t - Train Loss: 0.022088,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [24960/60000 (42%)]\t - Train Loss: 0.004889,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [25600/60000 (43%)]\t - Train Loss: 0.009014,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [26240/60000 (44%)]\t - Train Loss: 0.028392,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [26880/60000 (45%)]\t - Train Loss: 0.166940,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [27520/60000 (46%)]\t - Train Loss: 0.115841,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [28160/60000 (47%)]\t - Train Loss: 0.044008,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [28800/60000 (48%)]\t - Train Loss: 0.024937,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [29440/60000 (49%)]\t - Train Loss: 0.039572,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [30080/60000 (50%)]\t - Train Loss: 0.027014,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [30720/60000 (51%)]\t - Train Loss: 0.006207,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [31360/60000 (52%)]\t - Train Loss: 0.032578,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [32000/60000 (53%)]\t - Train Loss: 0.062110,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [32640/60000 (54%)]\t - Train Loss: 0.044377,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [33280/60000 (55%)]\t - Train Loss: 0.063302,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [33920/60000 (57%)]\t - Train Loss: 0.001190,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [34560/60000 (58%)]\t - Train Loss: 0.004301,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [35200/60000 (59%)]\t - Train Loss: 0.193908,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [35840/60000 (60%)]\t - Train Loss: 0.048825,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [36480/60000 (61%)]\t - Train Loss: 0.007351,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [37120/60000 (62%)]\t - Train Loss: 0.016680,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [37760/60000 (63%)]\t - Train Loss: 0.030995,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [38400/60000 (64%)]\t - Train Loss: 0.082250,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [39040/60000 (65%)]\t - Train Loss: 0.007134,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [39680/60000 (66%)]\t - Train Loss: 0.043349,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [40320/60000 (67%)]\t - Train Loss: 0.077062,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [40960/60000 (68%)]\t - Train Loss: 0.077565,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [41600/60000 (69%)]\t - Train Loss: 0.034141,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [42240/60000 (70%)]\t - Train Loss: 0.044164,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [42880/60000 (71%)]\t - Train Loss: 0.017133,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [43520/60000 (72%)]\t - Train Loss: 0.081826,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [44160/60000 (74%)]\t - Train Loss: 0.029212,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [44800/60000 (75%)]\t - Train Loss: 0.058543,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [45440/60000 (76%)]\t - Train Loss: 0.057007,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [46080/60000 (77%)]\t - Train Loss: 0.092304,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [46720/60000 (78%)]\t - Train Loss: 0.068812,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [47360/60000 (79%)]\t - Train Loss: 0.194919,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [48000/60000 (80%)]\t - Train Loss: 0.041527,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [48640/60000 (81%)]\t - Train Loss: 0.022769,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [49280/60000 (82%)]\t - Train Loss: 0.007241,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [49920/60000 (83%)]\t - Train Loss: 0.032656,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [50560/60000 (84%)]\t - Train Loss: 0.071661,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [51200/60000 (85%)]\t - Train Loss: 0.113516,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [51840/60000 (86%)]\t - Train Loss: 0.003308,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [52480/60000 (87%)]\t - Train Loss: 0.004619,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [53120/60000 (88%)]\t - Train Loss: 0.027843,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [53760/60000 (90%)]\t - Train Loss: 0.084205,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [54400/60000 (91%)]\t - Train Loss: 0.016452,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [55040/60000 (92%)]\t - Train Loss: 0.006700,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [55680/60000 (93%)]\t - Train Loss: 0.073527,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [56320/60000 (94%)]\t - Train Loss: 0.028192,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [56960/60000 (95%)]\t - Train Loss: 0.008758,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [57600/60000 (96%)]\t - Train Loss: 0.052015,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [58240/60000 (97%)]\t - Train Loss: 0.041319,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [58880/60000 (98%)]\t - Train Loss: 0.067881,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 3 [59520/60000 (99%)]\t - Train Loss: 0.000805,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Test set: Average loss: 0.0362, Accuracy: 98.78% (9878/10000)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [0/60000 (0%)]\t - Train Loss: 0.040501,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [640/60000 (1%)]\t - Train Loss: 0.009207,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [1280/60000 (2%)]\t - Train Loss: 0.006821,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [1920/60000 (3%)]\t - Train Loss: 0.071139,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [2560/60000 (4%)]\t - Train Loss: 0.027392,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [3200/60000 (5%)]\t - Train Loss: 0.060043,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [3840/60000 (6%)]\t - Train Loss: 0.004428,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [4480/60000 (7%)]\t - Train Loss: 0.063516,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [5120/60000 (9%)]\t - Train Loss: 0.066482,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [5760/60000 (10%)]\t - Train Loss: 0.118384,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [6400/60000 (11%)]\t - Train Loss: 0.085473,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [7040/60000 (12%)]\t - Train Loss: 0.138974,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [7680/60000 (13%)]\t - Train Loss: 0.056517,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [8320/60000 (14%)]\t - Train Loss: 0.015961,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [8960/60000 (15%)]\t - Train Loss: 0.134784,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [9600/60000 (16%)]\t - Train Loss: 0.001624,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [10240/60000 (17%)]\t - Train Loss: 0.159659,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [10880/60000 (18%)]\t - Train Loss: 0.001375,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [11520/60000 (19%)]\t - Train Loss: 0.114099,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [12160/60000 (20%)]\t - Train Loss: 0.093865,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [12800/60000 (21%)]\t - Train Loss: 0.109048,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [13440/60000 (22%)]\t - Train Loss: 0.033095,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [14080/60000 (23%)]\t - Train Loss: 0.005538,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [14720/60000 (25%)]\t - Train Loss: 0.024354,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [15360/60000 (26%)]\t - Train Loss: 0.001696,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [16000/60000 (27%)]\t - Train Loss: 0.026228,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [16640/60000 (28%)]\t - Train Loss: 0.117974,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [17280/60000 (29%)]\t - Train Loss: 0.003462,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [17920/60000 (30%)]\t - Train Loss: 0.034008,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [18560/60000 (31%)]\t - Train Loss: 0.015614,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [19200/60000 (32%)]\t - Train Loss: 0.024415,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [19840/60000 (33%)]\t - Train Loss: 0.046211,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [20480/60000 (34%)]\t - Train Loss: 0.026559,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [21120/60000 (35%)]\t - Train Loss: 0.122586,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [21760/60000 (36%)]\t - Train Loss: 0.006841,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [22400/60000 (37%)]\t - Train Loss: 0.008677,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [23040/60000 (38%)]\t - Train Loss: 0.052304,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [23680/60000 (39%)]\t - Train Loss: 0.035129,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [24320/60000 (41%)]\t - Train Loss: 0.000721,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [24960/60000 (42%)]\t - Train Loss: 0.004048,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [25600/60000 (43%)]\t - Train Loss: 0.004533,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [26240/60000 (44%)]\t - Train Loss: 0.025667,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [26880/60000 (45%)]\t - Train Loss: 0.034266,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [27520/60000 (46%)]\t - Train Loss: 0.073651,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [28160/60000 (47%)]\t - Train Loss: 0.028256,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [28800/60000 (48%)]\t - Train Loss: 0.013876,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [29440/60000 (49%)]\t - Train Loss: 0.043553,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [30080/60000 (50%)]\t - Train Loss: 0.017971,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [30720/60000 (51%)]\t - Train Loss: 0.068931,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [31360/60000 (52%)]\t - Train Loss: 0.036964,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [32000/60000 (53%)]\t - Train Loss: 0.011546,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [32640/60000 (54%)]\t - Train Loss: 0.035580,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [33280/60000 (55%)]\t - Train Loss: 0.105320,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [33920/60000 (57%)]\t - Train Loss: 0.008221,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [34560/60000 (58%)]\t - Train Loss: 0.001792,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [35200/60000 (59%)]\t - Train Loss: 0.114124,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [35840/60000 (60%)]\t - Train Loss: 0.024589,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [36480/60000 (61%)]\t - Train Loss: 0.013836,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [37120/60000 (62%)]\t - Train Loss: 0.019775,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [37760/60000 (63%)]\t - Train Loss: 0.121043,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [38400/60000 (64%)]\t - Train Loss: 0.241678,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [39040/60000 (65%)]\t - Train Loss: 0.009076,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [39680/60000 (66%)]\t - Train Loss: 0.081789,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [40320/60000 (67%)]\t - Train Loss: 0.027778,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [40960/60000 (68%)]\t - Train Loss: 0.100999,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [41600/60000 (69%)]\t - Train Loss: 0.009217,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [42240/60000 (70%)]\t - Train Loss: 0.014462,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [42880/60000 (71%)]\t - Train Loss: 0.015872,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [43520/60000 (72%)]\t - Train Loss: 0.051110,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [44160/60000 (74%)]\t - Train Loss: 0.002307,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [44800/60000 (75%)]\t - Train Loss: 0.022087,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [45440/60000 (76%)]\t - Train Loss: 0.088699,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [46080/60000 (77%)]\t - Train Loss: 0.090097,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [46720/60000 (78%)]\t - Train Loss: 0.081999,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [47360/60000 (79%)]\t - Train Loss: 0.075756,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [48000/60000 (80%)]\t - Train Loss: 0.036456,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [48640/60000 (81%)]\t - Train Loss: 0.015211,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [49280/60000 (82%)]\t - Train Loss: 0.005739,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [49920/60000 (83%)]\t - Train Loss: 0.038487,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [50560/60000 (84%)]\t - Train Loss: 0.042931,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [51200/60000 (85%)]\t - Train Loss: 0.159356,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [51840/60000 (86%)]\t - Train Loss: 0.005345,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [52480/60000 (87%)]\t - Train Loss: 0.017647,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [53120/60000 (88%)]\t - Train Loss: 0.025903,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [53760/60000 (90%)]\t - Train Loss: 0.052123,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [54400/60000 (91%)]\t - Train Loss: 0.039775,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [55040/60000 (92%)]\t - Train Loss: 0.004197,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [55680/60000 (93%)]\t - Train Loss: 0.043436,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [56320/60000 (94%)]\t - Train Loss: 0.020933,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [56960/60000 (95%)]\t - Train Loss: 0.013341,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [57600/60000 (96%)]\t - Train Loss: 0.061563,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [58240/60000 (97%)]\t - Train Loss: 0.026175,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [58880/60000 (98%)]\t - Train Loss: 0.000664,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 4 [59520/60000 (99%)]\t - Train Loss: 0.000140,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Test set: Average loss: 0.0305, Accuracy: 98.97% (9897/10000)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [0/60000 (0%)]\t - Train Loss: 0.047559,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [640/60000 (1%)]\t - Train Loss: 0.025351,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [1280/60000 (2%)]\t - Train Loss: 0.014788,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [1920/60000 (3%)]\t - Train Loss: 0.070722,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [2560/60000 (4%)]\t - Train Loss: 0.009325,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [3200/60000 (5%)]\t - Train Loss: 0.012957,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [3840/60000 (6%)]\t - Train Loss: 0.003318,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [4480/60000 (7%)]\t - Train Loss: 0.021359,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [5120/60000 (9%)]\t - Train Loss: 0.112027,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [5760/60000 (10%)]\t - Train Loss: 0.093672,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [6400/60000 (11%)]\t - Train Loss: 0.076813,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [7040/60000 (12%)]\t - Train Loss: 0.089859,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [7680/60000 (13%)]\t - Train Loss: 0.004754,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [8320/60000 (14%)]\t - Train Loss: 0.007266,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [8960/60000 (15%)]\t - Train Loss: 0.063570,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [9600/60000 (16%)]\t - Train Loss: 0.026415,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [10240/60000 (17%)]\t - Train Loss: 0.101716,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [10880/60000 (18%)]\t - Train Loss: 0.015175,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [11520/60000 (19%)]\t - Train Loss: 0.028317,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [12160/60000 (20%)]\t - Train Loss: 0.016818,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [12800/60000 (21%)]\t - Train Loss: 0.078142,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [13440/60000 (22%)]\t - Train Loss: 0.003467,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [14080/60000 (23%)]\t - Train Loss: 0.003707,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [14720/60000 (25%)]\t - Train Loss: 0.059259,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [15360/60000 (26%)]\t - Train Loss: 0.017021,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [16000/60000 (27%)]\t - Train Loss: 0.011423,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [16640/60000 (28%)]\t - Train Loss: 0.077011,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [17280/60000 (29%)]\t - Train Loss: 0.009682,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [17920/60000 (30%)]\t - Train Loss: 0.028985,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [18560/60000 (31%)]\t - Train Loss: 0.006745,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [19200/60000 (32%)]\t - Train Loss: 0.069341,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [19840/60000 (33%)]\t - Train Loss: 0.173574,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [20480/60000 (34%)]\t - Train Loss: 0.000454,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [21120/60000 (35%)]\t - Train Loss: 0.092404,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [21760/60000 (36%)]\t - Train Loss: 0.022065,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [22400/60000 (37%)]\t - Train Loss: 0.003331,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [23040/60000 (38%)]\t - Train Loss: 0.108191,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [23680/60000 (39%)]\t - Train Loss: 0.044716,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [24320/60000 (41%)]\t - Train Loss: 0.000114,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [24960/60000 (42%)]\t - Train Loss: 0.000824,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [25600/60000 (43%)]\t - Train Loss: 0.004573,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [26240/60000 (44%)]\t - Train Loss: 0.009338,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [26880/60000 (45%)]\t - Train Loss: 0.121087,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [27520/60000 (46%)]\t - Train Loss: 0.160635,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [28160/60000 (47%)]\t - Train Loss: 0.063143,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [28800/60000 (48%)]\t - Train Loss: 0.025606,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [29440/60000 (49%)]\t - Train Loss: 0.047610,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [30080/60000 (50%)]\t - Train Loss: 0.013477,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [30720/60000 (51%)]\t - Train Loss: 0.060667,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [31360/60000 (52%)]\t - Train Loss: 0.092714,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [32000/60000 (53%)]\t - Train Loss: 0.030245,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [32640/60000 (54%)]\t - Train Loss: 0.039134,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [33280/60000 (55%)]\t - Train Loss: 0.009720,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [33920/60000 (57%)]\t - Train Loss: 0.002026,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [34560/60000 (58%)]\t - Train Loss: 0.004298,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [35200/60000 (59%)]\t - Train Loss: 0.119813,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [35840/60000 (60%)]\t - Train Loss: 0.116836,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [36480/60000 (61%)]\t - Train Loss: 0.012294,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [37120/60000 (62%)]\t - Train Loss: 0.067092,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [37760/60000 (63%)]\t - Train Loss: 0.061892,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [38400/60000 (64%)]\t - Train Loss: 0.114619,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [39040/60000 (65%)]\t - Train Loss: 0.005758,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [39680/60000 (66%)]\t - Train Loss: 0.030495,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [40320/60000 (67%)]\t - Train Loss: 0.014965,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [40960/60000 (68%)]\t - Train Loss: 0.084801,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [41600/60000 (69%)]\t - Train Loss: 0.006267,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [42240/60000 (70%)]\t - Train Loss: 0.034980,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [42880/60000 (71%)]\t - Train Loss: 0.007220,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [43520/60000 (72%)]\t - Train Loss: 0.049036,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [44160/60000 (74%)]\t - Train Loss: 0.003399,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [44800/60000 (75%)]\t - Train Loss: 0.051134,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [45440/60000 (76%)]\t - Train Loss: 0.034411,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [46080/60000 (77%)]\t - Train Loss: 0.117575,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [46720/60000 (78%)]\t - Train Loss: 0.121943,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [47360/60000 (79%)]\t - Train Loss: 0.068476,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [48000/60000 (80%)]\t - Train Loss: 0.008368,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [48640/60000 (81%)]\t - Train Loss: 0.028512,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [49280/60000 (82%)]\t - Train Loss: 0.001211,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [49920/60000 (83%)]\t - Train Loss: 0.045363,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [50560/60000 (84%)]\t - Train Loss: 0.023051,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [51200/60000 (85%)]\t - Train Loss: 0.192470,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [51840/60000 (86%)]\t - Train Loss: 0.004567,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [52480/60000 (87%)]\t - Train Loss: 0.000207,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [53120/60000 (88%)]\t - Train Loss: 0.024115,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [53760/60000 (90%)]\t - Train Loss: 0.050342,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [54400/60000 (91%)]\t - Train Loss: 0.032615,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [55040/60000 (92%)]\t - Train Loss: 0.007504,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [55680/60000 (93%)]\t - Train Loss: 0.114156,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [56320/60000 (94%)]\t - Train Loss: 0.012539,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [56960/60000 (95%)]\t - Train Loss: 0.002240,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [57600/60000 (96%)]\t - Train Loss: 0.046242,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [58240/60000 (97%)]\t - Train Loss: 0.010101,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [58880/60000 (98%)]\t - Train Loss: 0.007326,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Train Epoch: 5 [59520/60000 (99%)]\t - Train Loss: 0.000677,\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m Test set: Average loss: 0.0310, Accuracy: 99.02% (9902/10000)\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 2023-04-03 06:48:57,385 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 2023-04-03 06:48:57,385 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf |\u001b[0m 2023-04-03 06:48:57,385 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "\u001b[36mhj714uvbww-algo-1-75lkf exited with code 0\n",
      "\u001b[0mAborting on container exit...\n",
      "===== Job Complete =====\n"
     ]
    }
   ],
   "source": [
    "create_experiment(experiment_name)\n",
    "job_name = create_trial(experiment_name)\n",
    "\n",
    "\n",
    "estimator.fit(\n",
    "    inputs={'training': s3_data_path}, \n",
    "    job_name=job_name,\n",
    "    experiment_config={\n",
    "      'TrialName': job_name,\n",
    "      'TrialComponentDisplayName': job_name,\n",
    "    },\n",
    "    wait=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_session.logs_for_job(job_name=job_name, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Debugger Profiling Report\n",
    "The profiling report rule will create an html report `profiler-report.html` with a summary of builtin rules and recommenades of next steps. You can find this report in your S3 bucket. For more information about how to download and open the Debugger profiling report, see [SageMaker Debugger Profiling Report](https://docs.aws.amazon.com/sagemaker/latest/dg/debugger-profiling-report.html) in the SageMaker developer guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You will find the profiler report in s3://sagemaker-us-west-2-322537213286/mnist-exp1-0401-07381680334715/rule-output\n"
     ]
    }
   ],
   "source": [
    "rule_output_path = estimator.output_path + estimator.latest_training_job.job_name + \"/rule-output\"\n",
    "print(f\"You will find the profiler report in {rule_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE profiler-reports/\n",
      "2023-04-01 07:48:11     351823 profiler-report.html\n",
      "2023-04-01 07:48:11     196930 profiler-report.ipynb\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls {rule_output_path}/ProfilerReport/profiler-output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_dir = './output'\n",
    "!rm -rf $output_dir\n",
    "\n",
    "profile_output = output_dir+'/ProfilerReport'\n",
    "\n",
    "if not os.path.exists(profile_output):\n",
    "    os.makedirs(profile_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-west-2-322537213286/mnist-exp1-0401-07381680334715/rule-output/ProfilerReport/profiler-output/profiler-reports/CPUBottleneck.json to output/ProfilerReport/profiler-reports/CPUBottleneck.json\n",
      "download: s3://sagemaker-us-west-2-322537213286/mnist-exp1-0401-07381680334715/rule-output/ProfilerReport/profiler-output/profiler-reports/BatchSize.json to output/ProfilerReport/profiler-reports/BatchSize.json\n",
      "download: s3://sagemaker-us-west-2-322537213286/mnist-exp1-0401-07381680334715/rule-output/ProfilerReport/profiler-output/profiler-report.html to output/ProfilerReport/profiler-report.html\n",
      "download: s3://sagemaker-us-west-2-322537213286/mnist-exp1-0401-07381680334715/rule-output/ProfilerReport/profiler-output/profiler-reports/LowGPUUtilization.json to output/ProfilerReport/profiler-reports/LowGPUUtilization.json\n",
      "download: s3://sagemaker-us-west-2-322537213286/mnist-exp1-0401-07381680334715/rule-output/ProfilerReport/profiler-output/profiler-reports/IOBottleneck.json to output/ProfilerReport/profiler-reports/IOBottleneck.json\n",
      "download: s3://sagemaker-us-west-2-322537213286/mnist-exp1-0401-07381680334715/rule-output/ProfilerReport/profiler-output/profiler-reports/GPUMemoryIncrease.json to output/ProfilerReport/profiler-reports/GPUMemoryIncrease.json\n",
      "download: s3://sagemaker-us-west-2-322537213286/mnist-exp1-0401-07381680334715/rule-output/ProfilerReport/profiler-output/profiler-report.ipynb to output/ProfilerReport/profiler-report.ipynb\n",
      "download: s3://sagemaker-us-west-2-322537213286/mnist-exp1-0401-07381680334715/rule-output/ProfilerReport/profiler-output/profiler-reports/LoadBalancing.json to output/ProfilerReport/profiler-reports/LoadBalancing.json\n",
      "download: s3://sagemaker-us-west-2-322537213286/mnist-exp1-0401-07381680334715/rule-output/ProfilerReport/profiler-output/profiler-reports/OverallFrameworkMetrics.json to output/ProfilerReport/profiler-reports/OverallFrameworkMetrics.json\n",
      "download: s3://sagemaker-us-west-2-322537213286/mnist-exp1-0401-07381680334715/rule-output/ProfilerReport/profiler-output/profiler-reports/Dataloader.json to output/ProfilerReport/profiler-reports/Dataloader.json\n",
      "download: s3://sagemaker-us-west-2-322537213286/mnist-exp1-0401-07381680334715/rule-output/ProfilerReport/profiler-output/profiler-reports/MaxInitializationTime.json to output/ProfilerReport/profiler-reports/MaxInitializationTime.json\n",
      "download: s3://sagemaker-us-west-2-322537213286/mnist-exp1-0401-07381680334715/rule-output/ProfilerReport/profiler-output/profiler-reports/OverallSystemUsage.json to output/ProfilerReport/profiler-reports/OverallSystemUsage.json\n",
      "download: s3://sagemaker-us-west-2-322537213286/mnist-exp1-0401-07381680334715/rule-output/ProfilerReport/profiler-output/profiler-reports/StepOutlier.json to output/ProfilerReport/profiler-reports/StepOutlier.json\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp {rule_output_path}/ProfilerReport/profiler-output/ {output_dir}/ProfilerReport/ --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>ProfilerReport : <a href=\"./output/ProfilerReport/profiler-report.html\">Profiler Report</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>ProfilerReport : <a href=\"{}profiler-report.html\">Profiler Report</a></b>'.format(output_dir+\"/ProfilerReport/\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
